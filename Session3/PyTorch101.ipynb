{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EVA4 S3 PyTorch101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sushmitha-Katti/EVA-4/blob/master/week3/PyTorch101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYM3ylzYo1Q",
        "colab_type": "text"
      },
      "source": [
        "![PyTorch](https://devblogs.nvidia.com/wp-content/uploads/2017/04/pytorch-logo-dark.png)\n",
        "\n",
        "An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVqSTrWY6oz",
        "colab_type": "text"
      },
      "source": [
        "# Tensor - Pytorch's core data structure\n",
        "\n",
        "In Python we can create lists, lists of lists, lists of lists and so on. In NumPy there is a `numpy.ndarray` which represents `n`- dimensional array. In math there is a special name for the generalization of vectors and matrices to a higher dimensional space - a tensor\n",
        "\n",
        "Tensor is an entity with a defined number of dimensions called an order (rank). \n",
        "\n",
        "**Scalar** can be considered as a rank-0-tensor. \n",
        "\n",
        "**Vector** can be introduced as a rank-1-tensor. \n",
        "\n",
        "**Matrices** can be considered as a rank-2-tensor.\n",
        "\n",
        "# Tensor Basics\n",
        "\n",
        "Let's import the torch module first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-oFVL2tYYqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxuR5IEaFJa",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Creation\n",
        "Let's view examples of matrices and tensors generation\n",
        "\n",
        "2-dimensional (rank-2) tensor of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897JQc25aD3E",
        "colab_type": "code",
        "outputId": "a13a50b8-a4c3-4088-eedb-9ee96bb37aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "torch.zeros(3, 4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pujmpEBhaPRA",
        "colab_type": "text"
      },
      "source": [
        "Random rank-3 tensor:\n",
        "_read the print below and convince yourself how this is a rank-3-tensor and learn what those 2, 3, 4 values are there for_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBrznTNhaOL7",
        "colab_type": "code",
        "outputId": "dd651463-2bf9-4359-8c79-ccc94a47b023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "torch.rand(2, 3, 4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0987, 0.7809, 0.8459, 0.6211],\n",
              "         [0.0461, 0.9516, 0.6230, 0.9399],\n",
              "         [0.4196, 0.6397, 0.5269, 0.5944]],\n",
              "\n",
              "        [[0.2695, 0.2468, 0.2180, 0.8245],\n",
              "         [0.2698, 0.6479, 0.5070, 0.6949],\n",
              "         [0.7116, 0.8713, 0.6034, 0.8120]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO4fIr15asdi",
        "colab_type": "text"
      },
      "source": [
        "I am hoping you have noticed 4-elements in a row, 3 rows making one block and there are 2 blocks. \n",
        "\n",
        "Random rank-4-tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCLvMGCaVZ3",
        "colab_type": "code",
        "outputId": "d5c01ff4-23be-4084-a143-b3e639ae1d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "torch.rand(2, 2, 2, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.4238, 0.5172, 0.5660],\n",
              "          [0.7864, 0.2312, 0.8998]],\n",
              "\n",
              "         [[0.2318, 0.1947, 0.7657],\n",
              "          [0.6428, 0.7590, 0.3680]]],\n",
              "\n",
              "\n",
              "        [[[0.0699, 0.9107, 0.4747],\n",
              "          [0.0936, 0.4158, 0.0542]],\n",
              "\n",
              "         [[0.0143, 0.0524, 0.7135],\n",
              "          [0.9181, 0.2190, 0.3682]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6VL68s3AMV",
        "colab_type": "text"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "How many dimensions are there in a tensor defined as below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IbXcI3x3Ibt",
        "colab_type": "code",
        "outputId": "9680220e-c9f5-413e-9141-5a5763cc922a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.rand(1, 1, 1, 1)\n",
        "\n",
        "#Solution : 4 dimensions\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.7166]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgpqssRa9jF",
        "colab_type": "text"
      },
      "source": [
        ".\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are many more ways to create tensor using some restrictions on values it should contatn - for the full reference, please follow the [official docs](https://pytorch.org/docs/stable/torch.html#creation-ops). \n",
        "\n",
        "\n",
        ".\n",
        "---\n",
        "\n",
        "\n",
        "# Python / NumPy / Pytorch interoperability\n",
        "\n",
        "You can create tensors from python as well as numpy arrays. You can also convert torch tensors to numpy arrays. So, the interoperability between torch and numpy is pretty good. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipgpeWPfa6gE",
        "colab_type": "code",
        "outputId": "ef46a535-ebe6-45a9-a12c-8864a9c22a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Simple Python List\n",
        "python_list = [1, 2]\n",
        "\n",
        "# Create a numpy array from python list\n",
        "numpy_array = np.array(python_list)\n",
        "\n",
        "# Create a torch Tensor from python list\n",
        "tensor_from_list = torch.tensor(python_list)\n",
        "\n",
        "# Create a torch Tensor from Numpy array\n",
        "tensor_from_array = torch.tensor(numpy_array)\n",
        "\n",
        "# Another way to create a torch Tensor from Numpy array (share same storage)\n",
        "tensor_from_array_v2 = torch.from_numpy(numpy_array)\n",
        "\n",
        "# Convert torch tensor to numpy array\n",
        "array_from_tensor = tensor_from_array.numpy()\n",
        "\n",
        "print('List:   ', python_list)\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_list)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)\n",
        "print('Array:  ', array_from_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List:    [1, 2]\n",
            "Array:   [1 2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Array:   [1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_x-86B8gqik",
        "colab_type": "text"
      },
      "source": [
        "**Difference between** `torch.Tensor` **and** `torch.from_numpy`\n",
        "\n",
        "Pytorch aims to be an effective library for computations. What does it mean? It means that pytorch avoids memory copying if it can. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCKDGpygn_d",
        "colab_type": "code",
        "outputId": "a5a7fe2d-a6d2-4166-b16e-dd5b0f7f6570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "numpy_array[0] = 10\n",
        "\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2) #from_numpy- same memory different refferences\n",
        "                                        #torch.tensor-different memory different reference\n",
        "print(type(tensor_from_array_v2),type(tensor_from_array))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array:   [10  2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([10,  2])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CKtdNRM3SMp",
        "colab_type": "text"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Assume that we moved our complete (cats vs dogs) image dataset to numpy arrays. Then we use torch.from_numpy to convert these images to tensor. Then we apply a specific data augmentation strategy called \"CutOut\" which blocks a portion of the image directly on these tensors. What will happen to the accuracy of a model trained on this strategy compared to the one without this strategy? CutOut strategy is shown below: \n",
        "\n",
        "![CutOut](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSnSyN835AmtQPKQbPjDHX-FmshNilbtexX95cRGQPwl56QCGDn)\n",
        "\n",
        "Data Augumentation - Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Some Of the data agumentation techniques are crop,scale,rotate,flip,cut out. \n",
        "##CutOut \n",
        "\n",
        "This technique helps to avoid overfitting,improve the robustness and overall performance of convolutional neural networks.\n",
        "There is no additional computational cost as well.\n",
        "\n",
        "\n",
        "\n",
        "## Question 3:\n",
        "Why do you think we are observing this behavior?\n",
        "\n",
        "##Solution\n",
        "When we have less data this technique helps to increase the dataset by diversing the features of the same image.CutOut forces models to take more of the full image context into consideration,rather than focusing on a few key visualfeatures,which may not always be present\n",
        "Cutout Regularization will force the model to look at other details of the image.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZq7O-aihSOA",
        "colab_type": "text"
      },
      "source": [
        "We have two different ways to create tensor from its NumPy counterpart - one copies memory and another one shares the same underlying storage. It works in the opposite way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNFOwV8EhPwQ",
        "colab_type": "code",
        "outputId": "cf57f289-2fab-4dc4-aaa1-4145c4f67eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "array_from_tensor = tensor_from_array.numpy() #tensor_from_array id created from torch.tensor\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor) #.numpy shares same storage\n",
        "\n",
        "tensor_from_array[0] = 11\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)\n",
        "print(type(tensor_from_array))\n",
        "array_from_tensor[1] = 11\n",
        "print('Tensor: ', tensor_from_array)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor:  tensor([11,  2])\n",
            "Array:  [11  2]\n",
            "Tensor:  tensor([11,  2])\n",
            "Array:  [11  2]\n",
            "<class 'torch.Tensor'>\n",
            "Tensor:  tensor([11, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM9ytbvKhtCw",
        "colab_type": "text"
      },
      "source": [
        "## Data types\n",
        "\n",
        "The basic data type of all Deep Learning-related operations is float, but sometimes you may need something else. Pytorch support different number types for its tensors the same way NumPy does it - by specifying the data type on tensor creation or via casting. Ths full list of supported data types can be found [here](https://pytorch.org/docs/stable/tensors.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd6WkzJ4hpYi",
        "colab_type": "code",
        "outputId": "44af33eb-f027-4e19-f4f6-08cd9010b5fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "tensor = torch.zeros(2, 2)\n",
        "print('Tensor with default type: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.float16)\n",
        "print('Tensor with 16-bit float: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.int16)\n",
        "print('Tensor with integers: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.bool)\n",
        "print('Tensor with boolean data: ', tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor with default type:  tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Tensor with 16-bit float:  tensor([[0., 0.],\n",
            "        [0., 0.]], dtype=torch.float16)\n",
            "Tensor with integers:  tensor([[0, 0],\n",
            "        [0, 0]], dtype=torch.int16)\n",
            "Tensor with boolean data:  tensor([[False, False],\n",
            "        [False, False]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9F4Dkdr40TE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 4:\n",
        "We saw above that some times numpy and tensors share same storage and changing one changes the other. \n",
        "If we define a rank-2-tensor with ones (dtype of f16), and then convert it into a numpy data type using tensor.numpy() and store it in a variable called \"num\", and then we perform this operation `num = num * 0.5`, will the original tensor have 1.0s or 0.5s as its element values? \n",
        "\n",
        "##Solution:\n",
        "It remains as it is. So original tensor has 1s\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPUKRDwejSn0",
        "colab_type": "code",
        "outputId": "cfa838e6-ae9a-4757-f68c-91a23be09392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "a = torch.ones(3, 4)\n",
        "n = a.numpy()\n",
        "n = n*0.5\n",
        "print(a)             #Though both point to same memory, they are not same because, in python whenever its '=' sign means creating\n",
        "print(n)             #new object i.e in n = n*5 L.H.S is a new object that refers to different memory. Now 'a' is refering to same memory \n",
        "                    #But n reference has changed that's why it did  a didnot change. \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "[[0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]]\n",
            "**********************************\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "[[0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]]\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQt8Mmm51OE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: \n",
        "If the operation `num = num*5` is changed to `num[:] = num*5` will the original tensor have 1.0s or 0.5s as its element values? \n",
        "##Solution\n",
        "Original Tensor have 0.5s as its element values.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgQQJZPelc_l",
        "colab_type": "code",
        "outputId": "92540450-9fbd-4344-8be1-b1b3ededecc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "a = torch.ones(3, 4)\n",
        "n = a.numpy()\n",
        "n[:] =n* 0.5\n",
        "print(a)             #Same explaination, here n is refering to same memory. It didnot create new object. SO its value didnot change\n",
        "print(n)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000]])\n",
            "[[0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzh8UB8KiVmb",
        "colab_type": "text"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "Tensor provides access to its elements via the same `[]` operation as a regular python list or NumPy array. However, as you may recall from NumPy usage, the full power of math libraries is accessible only via vectorized operations, i.e. operations without explicit looping over all vector elements in python and using implicit optimized loops in C/C++/CUDA/Fortran/etc. available via special function calls. Pytorch employs the same paradigm and provides a wide range of vectorized operations. Let's take a look at some examples. \n",
        "\n",
        "Joining a list of tensors together with `torch.cat`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMaCDKPhiUAb",
        "colab_type": "code",
        "outputId": "bc0a7e9b-7197-402e-c826-925b38bd4a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "print(torch.cat((a, b), dim=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bj4aeE86zdH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: \n",
        "Is the transpose of concatenated a & b tensor on dimension 1, same as the contatenated tensor of a & b on dimension 0? \n",
        "\n",
        "## Solution:\n",
        "No. Their dimension is differnt.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-zBfMFhqTfc",
        "colab_type": "code",
        "outputId": "640146ed-ac32-42da-b262-7a12c6e70920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "print(torch.cat((a, b), dim=0))\n",
        "print(torch.transpose(torch.cat((a,b), dim=1),1,0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TXNne69j7LP",
        "colab_type": "text"
      },
      "source": [
        "Indexing with another tenxor/array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-Fsi4jVd6",
        "colab_type": "code",
        "outputId": "baba0b81-9d68-4fe7-ba1a-240077d7bc07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "indices = np.arange(0, 10) > 5\n",
        "print(\"array\",a)\n",
        "print(\"indices\",indices)\n",
        "print(\"a[indices]\",a[indices])\n",
        "\n",
        "print(\"************************************************\")\n",
        "indices = torch.arange(start=0, end=10) %5\n",
        "print(\"tensor-indices\",indices)\n",
        "print(\"a[indices]\",a[indices])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "indices [False False False False False False  True  True  True  True]\n",
            "a[indices] tensor([6, 7, 8, 9])\n",
            "************************************************\n",
            "tensor-indices tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "a[indices] tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4dnlu47WQu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 7:\n",
        "\n",
        "`a` is defined as `torch.arange(start=0, end=10)`. We will create `b` using the two operations as below. In both cases do we get the same value?\n",
        "\n",
        "\n",
        "1.   indices variable created by the modulo operation on arange between 0 and 10. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "2.   indices variable created by the modulo operation on arange betwenn 1 and 11. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "\n",
        "## Solution: \n",
        "Sometimes yes(till 5). But not all the time.\n",
        "If yes also only the values are same but not the order.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8sdlYsXsszK",
        "colab_type": "code",
        "outputId": "911421e6-ebcf-4649-df78-049e253ddbd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = torch.arange(start=0,end=10)\n",
        "indices1 = np.arange(0, 10) % 6\n",
        "indices2 = np.arange(1, 11) % 6\n",
        "print(a[indices1],a[indices2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 0, 1, 2, 3]) tensor([1, 2, 3, 4, 5, 0, 1, 2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ4ZCVsTk-KH",
        "colab_type": "text"
      },
      "source": [
        "What should we do if we have, say, rank-2-tensor and want to select only some rows?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtRpotjkt1q",
        "colab_type": "code",
        "outputId": "4d67adb5-db44-4285-fd61-e1e1290aa60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "tensor = torch.rand((5, 3))\n",
        "rows = torch.tensor([0, 2])\n",
        "print(tensor)\n",
        "print(rows)\n",
        "print(tensor[rows])\n",
        "print(tensor[0:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8120, 0.7163, 0.8440],\n",
            "        [0.6048, 0.1781, 0.6927],\n",
            "        [0.8233, 0.8755, 0.8805],\n",
            "        [0.4594, 0.2830, 0.8046],\n",
            "        [0.6217, 0.0209, 0.5418]])\n",
            "tensor([0, 2])\n",
            "tensor([[0.8120, 0.7163, 0.8440],\n",
            "        [0.8233, 0.8755, 0.8805]])\n",
            "tensor([[0.8120, 0.7163, 0.8440],\n",
            "        [0.6048, 0.1781, 0.6927]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIJnr_N2_Qaf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 8: \n",
        "\n",
        "Consider a tensor defined as `torch.rand((6, 5))`. Is the shape of the new tensor created by taking the 0th, 2nd and 4th row of the old tensor same as the shape of the a newer tensor created by taking the 0th, 2nd and 4th row of the old tensor after transposing it by operation `torch.transpose(tensor, 0, 1)` ?\n",
        "\n",
        "## Solution:\n",
        "No\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU9K3SQDvMeX",
        "colab_type": "code",
        "outputId": "2a00419c-922f-43a6-b794-25f1df06934f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "tensor = torch.rand((6,5))\n",
        "rows = torch.tensor([0, 2, 4])\n",
        "new_tensor = tensor[rows] #Shape = 3*5\n",
        "new_tensor2 = torch.transpose(tensor, 0, 1)[rows] #Shape = 3*6\n",
        "print(new_tensor)\n",
        "print(new_tensor2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5905, 0.3325, 0.2013, 0.6247, 0.8747],\n",
            "        [0.9453, 0.9797, 0.6449, 0.8744, 0.1602],\n",
            "        [0.4711, 0.2072, 0.5989, 0.5237, 0.2588]])\n",
            "tensor([[0.5905, 0.7558, 0.9453, 0.4224, 0.4711, 0.4371],\n",
            "        [0.2013, 0.5041, 0.6449, 0.8076, 0.5989, 0.4190],\n",
            "        [0.8747, 0.8895, 0.1602, 0.1273, 0.2588, 0.6242]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDzFMkslU0b",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Shapes\n",
        "\n",
        "Reshaping a tensor is a frequently used operation. We can change the shape of a tensor without the memory copying overhead. There are two methods for that: `reshape` and `view`. \n",
        "\n",
        "The difference is the following: \n",
        "\n",
        "\n",
        "*   view tries to return a tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to [some reason](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view), it just fails. \n",
        "*   reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy\n",
        "\n",
        "Let's see with the help of an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClDkqLLlMJh",
        "colab_type": "code",
        "outputId": "667d2ea3-074a-4fcf-d684-ff49c7ace168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "tensor = torch.rand(2, 3, 4)\n",
        "print('Pointer to data: ', tensor.data_ptr())\n",
        "print('Shape: ', tensor.shape)\n",
        "\n",
        "reshaped = tensor.reshape(24)\n",
        "\n",
        "view = tensor.view(3, 2, 4)\n",
        "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
        "print('Reshaped tensor shape ', reshaped.shape)\n",
        "\n",
        "print('Viewed tensor - pointer to data', view.data_ptr())\n",
        "print('Viewed tensor shape ', view.shape)\n",
        "\n",
        "assert tensor.data_ptr() == view.data_ptr() #An assertion is a sanity-check that you can turn on or turn off when you are done with your testing of the program.\n",
        "#The easiest way to think of an assertion is to liken it to a raise-if statement (or to be more accurate, a raise-if-not statement). An expression is tested, and if the result comes up false, an exception is raised.\n",
        "\n",
        "\n",
        "#flat converts ndarray to 1D array. It returns flatiter object which can be printed using loop\n",
        "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
        "\n",
        "#np. all() is a function that returns True when all elements of ndarray passed to the first parameter are True , and returns False otherwise. \n",
        "print('Original stride: ', tensor.stride()) #Stride is the jump necessary to go from one element to the next one in the specified dimension \n",
        "print('Reshaped stride: ', reshaped.stride()) #Stride not understood properly..........\n",
        "print('Viewed stride: ', view.stride())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pointer to data:  93812864\n",
            "Shape:  torch.Size([2, 3, 4])\n",
            "Reshaped tensor - pointer to data 93812864\n",
            "Reshaped tensor shape  torch.Size([24])\n",
            "Viewed tensor - pointer to data 93812864\n",
            "Viewed tensor shape  torch.Size([3, 2, 4])\n",
            "Original stride:  (12, 4, 1)\n",
            "Reshaped stride:  (1,)\n",
            "Viewed stride:  (8, 4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIN5jSppm4yC",
        "colab_type": "text"
      },
      "source": [
        "The basic rule about reshaping the tensor is definitely that you cannot change the total number of elements in it, so the product of all tensor's dimensions should always be the same. It gives us the ability to avoid specifying one dimension when reshaping the tensor - Pytorch can calculate it for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3D19ERFmzOl",
        "colab_type": "code",
        "outputId": "c03b4011-bc07-4902-813b-c7ad84102678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "\n",
        "print(tensor.reshape(3, 2, 4).shape)\n",
        "print(tensor.reshape(3, 2, -1).shape)\n",
        "\n",
        "print(tensor.reshape(3, -1, 4).shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObgCQKUiETak",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 9:\n",
        "\n",
        "Consider a tensor `a` created with [1, 2, 3] and [1, 2, 3] of size (2, 3) is reshaped with operation `.reshape(-1, 2)`. Also consider a tensor `b` created with [[2, 1]] and of size (1, 2), later operated with `view(2, -1)` operation. \n",
        "\n",
        "If we do a dot product of a and b (using `torch.mm`) and perform the sum of all the elements (using `torch.sum`) what do we get? (enter int value without any decimal point in the quiz)\n",
        "\n",
        "## Solution :\n",
        "19\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XrO0iJGMYB2",
        "colab_type": "code",
        "outputId": "1275abd9-f071-47a7-81d5-533770c77a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "a = torch.tensor([[1,2,3],[1,2,3]]).reshape(-1,2)\n",
        "print(a.shape)\n",
        "b = torch.tensor([[2,1]]).view(2,-1)\n",
        "print(b.shape)\n",
        "print(torch.sum(torch.mm(a,b)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2])\n",
            "torch.Size([2, 1])\n",
            "tensor(18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mza7QPg3ndeV",
        "colab_type": "text"
      },
      "source": [
        "**Alternative ways to view tensors** - `expand` or `expand_as`.\n",
        "\n",
        "\n",
        "\n",
        "*   `expand` - requires the desired shape as an input\n",
        "*   `expand_as` - uses the shape of another tensor\n",
        "\n",
        "These operations \"repeat\" tensor's values along the specified axes without actually copying the data. \n",
        "\n",
        "As the documentation says, expand:\n",
        "\n",
        "\n",
        "> returns a new view of the self tensor with singleton dimensions expanded to a larger size. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. \n",
        "\n",
        "**Use case:**\n",
        "\n",
        "\n",
        "\n",
        "*   index multi-channel tensor with single-channel mask - imagine a color image with 3 channels (RGB) and binary mask for the area of interest on that image. We cannot index the image with this kind of mask directly since the dimensions are different, but we can use `expand_as` operation to create a view of the mask that has the same dimensions as the image we want to apply it to, but has not copied the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzJXaQskOaCo",
        "colab_type": "code",
        "outputId": "df878903-c5f9-4ff6-8e01-85ec37c6fa8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "a = torch.tensor([[1],[3]]) #the single ton matrix shape can be alterd, other shape may be altered, if any row/column is not single ton then this doesnot work\n",
        "a.expand(2,3)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1],\n",
              "        [3, 3, 3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz33E-V7nPQT",
        "colab_type": "code",
        "outputId": "c4c184f3-c8da-45bd-baf4-79c4520b989a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "\n",
        "#Didnot understood what it does ????????????????????????????\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create a black image\n",
        "image = torch.zeros(size=(3, 256, 256), dtype=torch.int)\n",
        "\n",
        "# Leave the borders and make the rest of the image Green\n",
        "image[1, 18:256 - 18, 18:256 - 18] = 255\n",
        "\n",
        "# Create a mask of the same size\n",
        "mask = torch.zeros(size=(256, 256), dtype=torch.bool)\n",
        "\n",
        "# Assuming the green region in the original image is the Region of interest, change the mask to white for that area\n",
        "mask[18:256 - 18, 18:256 - 18] = 1\n",
        "\n",
        "# Create a view of the mask with the same dimensions as the original image\n",
        "mask_expanded = mask.expand_as(image)\n",
        "print(mask_expanded.shape)\n",
        "\n",
        "mask_np = mask_expanded.numpy().transpose(1, 2, 0) * 255\n",
        "image_np = image.numpy().transpose(1, 2, 0)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[0, mask] += 128\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[mask_expanded] += 140\n",
        "#image.clamp_(0, 255)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 256, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNfT/8Gu/+Nmt8vQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiXBx0k3ptOI",
        "colab_type": "text"
      },
      "source": [
        "In the example above, one can also find a couple of useful tricks:\n",
        "\n",
        "\n",
        "*   `clamp` method and function is a Pytorch's analogue of NumPy's `clip` function\n",
        "*   many operations on tensors have in-place form, that does not return modified data, but change values in the tensor. The in-place version of the operation has trailing underscore according to Pytorch's naming convension - in the exmaple above it is `clamp_`\n",
        "*   tensors have the same indexing as Numpy's arrays - one can use `:` seperated range, negative indexes and so on.\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Images and their representations\n",
        "\n",
        "Now, let's discuss images, their representations and how different Python librarties work with them. \n",
        "\n",
        "Probably, the most well-known library for image loading and simple processing is [Pillow](https://pillow.readthedocs.io/en/stable/). \n",
        "\n",
        "However, many people in deep learning area stick with OpenCV for image loading and processing with some usage of another libraries when it is justified by performance/functionality. This is because OpenCV is in general much faster than the other libraries. Here you can find a couple of benchmarks: \n",
        "\n",
        "*   https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading\n",
        "*   https://github.com/albumentations-team/albumentations#benchmarking-results\n",
        "\n",
        "To sum up the benchmarks above, there are two most common image formats, PNG and JPEGs. If your data is in PNG format - use OpenCV to read it. If it is in JPEG - use libturbojpeg. For image processing, use OpenCV if possible. _We will be using PIL a lot along with these._\n",
        "\n",
        "As you will read the code from others, you may find out that some of them use Pillow/something else to read data. You should know, that color image representations in OpenCV and other libraries are different - OpenCV uses \"BGR\" channel order, while others use \"RGB\" one. \n",
        "\n",
        "To change \"BRG\" <-> \"RGB\" the only thing we need to do it to change channel order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYv4sZMmpndu",
        "colab_type": "code",
        "outputId": "bd6ceac6-e2bb-40bc-8034-084a6ef204fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "\n",
        "bgr_image = cv2.imread('Mars.jfif') \n",
        "# remember to add your own image in case you run this block, if you want to use the same image, \n",
        "# download it from: \n",
        "print(bgr_image.shape)\n",
        "rgb_image = bgr_image[..., ::-1] #OPEN CV uses BGR representation, PIL uses RGB representation.\n",
        "print(rgb_image.shape)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(bgr_image)\n",
        "ax[1].imshow(rgb_image)\n",
        "plt.show()\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# fig, ax = plt.subplots(1, 2)\n",
        "# ax[0].cv2_imshow(bgr_image)\n",
        "# ax[1].cv2_imshow(rgb_image)\n",
        "# plt.show()\n",
        "# #cv2_imshow(rgb_image)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(450, 682, 3)\n",
            "(450, 682, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACHCAYAAADtJRlTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoi4TeNBGHle",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 10:\n",
        "\n",
        "Looking at the results above it can be said that the pixel values in the blue channels would be very small compared to red channel. True/False?\n",
        "# Solution:\n",
        "\n",
        "\n",
        "\n",
        "Yes. When converted to rgb r values are swapped with blue so blue will be more in 1 case\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ugFd57zNwa",
        "colab_type": "text"
      },
      "source": [
        "# Autograd\n",
        "\n",
        "Pytorch supports automatic differentiation. The module which implements this is called **AutoGrad**. It calculates the gradients and keeps track in forward and backward passes. For primitive tensors, you need to enable or disable it using the `required_grad` flag. But, for advanced tensors, it is enabled by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO1TydO5csV3",
        "colab_type": "code",
        "outputId": "53496ecd-4790-4edf-cf87-aac5d0593da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "sample = torch.ones((3, 3), requires_grad = True)\n",
        "#t = torch.ones((3, 3), requires_grad = True)\n",
        "print(sample)\n",
        "result = sample **  5  \n",
        "result +=4  \n",
        "b =result.sum()\n",
        "print(b)\n",
        "#b.backward()   //d(r)/d(sample) = \n",
        "# print(b)\n",
        "# result = sample *4\n",
        "# b = result.sum()\n",
        "# result *=  t*10\n",
        "# b =result.sum()\n",
        "\n",
        "b.backward(create_graph = True)\n",
        "#b.backward(create_graph = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(sample.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], requires_grad=True)\n",
            "tensor(45., grad_fn=<SumBackward0>)\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.],\n",
            "        [5., 5., 5.]], grad_fn=<CloneBackward>)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMOp4aiou6JR",
        "colab_type": "code",
        "outputId": "b12342b7-7de5-4deb-c931-5cee5f90eb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad = True)\n",
        "print(a)\n",
        "result = a * 5\n",
        "print(result)\n",
        "\n",
        "# grad can be implicitly created only for scalar outputs\n",
        "# so let's calculate the sum here so that the output becomes a scalar and we can apply a backward pass\n",
        "mean_result = result.sum()\n",
        "print(mean_result)\n",
        "# calculate gradient\n",
        "mean_result.backward()\n",
        "# print gradient of a\n",
        "print(a.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0841, 0.3649, 0.1252, 0.1546, 0.6832],\n",
            "        [0.2440, 0.3217, 0.1589, 0.3764, 0.2015],\n",
            "        [0.7608, 0.6018, 0.9185, 0.4745, 0.7665]], requires_grad=True)\n",
            "tensor([[0.4206, 1.8246, 0.6258, 0.7728, 3.4159],\n",
            "        [1.2200, 1.6083, 0.7945, 1.8820, 1.0076],\n",
            "        [3.8040, 3.0092, 4.5927, 2.3725, 3.8325]], grad_fn=<MulBackward0>)\n",
            "tensor(31.1829, grad_fn=<SumBackward0>)\n",
            "tensor([[5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0Amk2IGfLx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 11: \n",
        "\n",
        "Why the gradient of a is all 5s above?\n",
        "\n",
        "##Solution:\n",
        "\n",
        "Since the python auto grad keeps tracks of forward and backward passes. \n",
        "First a was initialized with required grad = True. It keeps tracks now\n",
        "Then a was multipled by 5 and assigned to result\n",
        "AS soon as it is backward\n",
        "a is differentiated\n",
        "So 5a by differentiation becomes 5. So it is 5.\n",
        "i.e. d(result)/d(a) = d(5*a)/d(a) = 5\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PDgGq2R0k7I",
        "colab_type": "text"
      },
      "source": [
        "As we see, Pytorch automagically calculated the gradient value for us. It looks to be the correct value - we multiplied an input by 5, so the gradient of this operation equals to 5.\n",
        "\n",
        "# Disabling Autograd for tensors\n",
        "\n",
        "We don't need to compute gradients for all the variables that are involved in the pipeline. The Pytorch API provides 2 ways to disable autograd.\n",
        "\n",
        "`detach` - returns a copy of the tensor with autograd disabled. This \n",
        "\n",
        "1.   copy is built on the same memory as the original tensor, so in-place size / stride / storage changes (such as resize_ / resizeas / set / transpose) modifications are not allowed.\n",
        "2.   torch.no_grad() - It is a context manager that allows you to guard a series of operations from autograd without creating new tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqVG9fQb0cLW",
        "colab_type": "code",
        "outputId": "c6ed97c3-b591-4a53-d11b-4b07ed050106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "detached_a = a.detach()\n",
        "detached_result = detached_a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "print(a.grad)\n",
        "print(detached_a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[10., 10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10., 10.],\n",
            "        [10., 10., 10., 10., 10.]])\n",
            "tensor([[0.2956, 0.6017, 0.0285, 0.3416, 0.7203],\n",
            "        [0.6045, 0.1914, 0.5328, 0.0754, 0.7585],\n",
            "        [0.6801, 0.2495, 0.3864, 0.3534, 0.8077]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqpch2Be02J7",
        "colab_type": "code",
        "outputId": "0a5ee8e8-81b8-46f0-c7fc-ac406657e928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    detached_result = a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjh2rYOPJUAZ",
        "colab_type": "text"
      },
      "source": [
        "# Custom Network\n",
        "\n",
        "A fully-connected ReLU network with one hidden layer and no biases, trained to predict y from x by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses PyTorch tensors to manually compute the forward pass, loss, and backward pass.\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGqVve6Bunef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtype = torch.float\n",
        "#device = torch.device(\"cpu\")\n",
        "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "#print(x.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nf5RaB104Vp",
        "colab_type": "code",
        "outputId": "c8a52d2d-4447-4a59-9d82-1a5bb282db29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# dtype = torch.float\n",
        "# #device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# # N is batch size; D_in is input dimension;\n",
        "# # H is hidden dimension; D_out is output dimension.\n",
        "# N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# # Create random input and output data\n",
        "# x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "# y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# # Randomly initialize weights\n",
        "# w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "# w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# #print(x.shape)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.mm(w1)\n",
        "    h_relu = h.clamp(min=0) #This is exactly acts as relu\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss          #X------> H-------> O\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2*(y_pred - y)\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "    grad_h = grad_h_relu.clone()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 8.91093520749564e-07\n",
            "1 8.911991926652263e-07\n",
            "2 8.890327762856032e-07\n",
            "3 8.869623115970171e-07\n",
            "4 8.862236882123398e-07\n",
            "5 8.872647185853566e-07\n",
            "6 8.850720405462198e-07\n",
            "7 8.873399792719283e-07\n",
            "8 8.881108897185186e-07\n",
            "9 8.891384482012654e-07\n",
            "10 8.826794442029495e-07\n",
            "11 8.796957899903646e-07\n",
            "12 8.813922818262654e-07\n",
            "13 8.808417533145985e-07\n",
            "14 8.75147065926285e-07\n",
            "15 8.73215014962625e-07\n",
            "16 8.699508953213808e-07\n",
            "17 8.719728157302598e-07\n",
            "18 8.718251365280594e-07\n",
            "19 8.718696449250274e-07\n",
            "20 8.691904440638609e-07\n",
            "21 8.741575925341749e-07\n",
            "22 8.730476679374988e-07\n",
            "23 8.788708782958565e-07\n",
            "24 8.758468084124615e-07\n",
            "25 8.732953347134753e-07\n",
            "26 8.789542107479065e-07\n",
            "27 8.804856861388544e-07\n",
            "28 8.804589697319898e-07\n",
            "29 8.795303756414796e-07\n",
            "30 8.786145144767943e-07\n",
            "31 8.756728675507475e-07\n",
            "32 8.754057034821017e-07\n",
            "33 8.792952144176525e-07\n",
            "34 8.813132694740489e-07\n",
            "35 8.773586728239025e-07\n",
            "36 8.773185413701867e-07\n",
            "37 8.699835234438069e-07\n",
            "38 8.709724852451473e-07\n",
            "39 8.730798981559929e-07\n",
            "40 8.725978659640532e-07\n",
            "41 8.732461083127419e-07\n",
            "42 8.735366918699583e-07\n",
            "43 8.73226440489816e-07\n",
            "44 8.716912134332233e-07\n",
            "45 8.706863354746019e-07\n",
            "46 8.732811238587601e-07\n",
            "47 8.707904726179549e-07\n",
            "48 8.745736295168172e-07\n",
            "49 8.71834686222428e-07\n",
            "50 8.718656090422883e-07\n",
            "51 8.719969173398567e-07\n",
            "52 8.684150429871806e-07\n",
            "53 8.699861950844934e-07\n",
            "54 8.715194894648448e-07\n",
            "55 8.647691061014484e-07\n",
            "56 8.67006406224391e-07\n",
            "57 8.650442282487347e-07\n",
            "58 8.689416404195072e-07\n",
            "59 8.693222071087803e-07\n",
            "60 8.625166856290889e-07\n",
            "61 8.605300081399037e-07\n",
            "62 8.610298891653656e-07\n",
            "63 8.614778153059888e-07\n",
            "64 8.616232776148536e-07\n",
            "65 8.595186500315322e-07\n",
            "66 8.604446293247747e-07\n",
            "67 8.5979968389438e-07\n",
            "68 8.543789817849756e-07\n",
            "69 8.553329280402977e-07\n",
            "70 8.559791240259074e-07\n",
            "71 8.584506758779753e-07\n",
            "72 8.658890351398441e-07\n",
            "73 8.639307793600892e-07\n",
            "74 8.617598723503761e-07\n",
            "75 8.618376909907965e-07\n",
            "76 8.580396979596117e-07\n",
            "77 8.611349926468392e-07\n",
            "78 8.612769875071535e-07\n",
            "79 8.605100560998835e-07\n",
            "80 8.609472956777608e-07\n",
            "81 8.647535878480994e-07\n",
            "82 8.65877609612653e-07\n",
            "83 8.642448392492952e-07\n",
            "84 8.652116321172798e-07\n",
            "85 8.609825385974545e-07\n",
            "86 8.614690614194842e-07\n",
            "87 8.610950317233801e-07\n",
            "88 8.572585556976264e-07\n",
            "89 8.573774721298832e-07\n",
            "90 8.586736157667474e-07\n",
            "91 8.606233450336731e-07\n",
            "92 8.609682708993205e-07\n",
            "93 8.588777973272954e-07\n",
            "94 8.594058726885123e-07\n",
            "95 8.605649099990842e-07\n",
            "96 8.619508662377484e-07\n",
            "97 8.610800819042197e-07\n",
            "98 8.556975217288709e-07\n",
            "99 8.555867907489301e-07\n",
            "100 8.566999554204813e-07\n",
            "101 8.577812309340516e-07\n",
            "102 8.506715971634549e-07\n",
            "103 8.512250246894837e-07\n",
            "104 8.464106144856487e-07\n",
            "105 8.476990274175478e-07\n",
            "106 8.44994644921826e-07\n",
            "107 8.473091952510003e-07\n",
            "108 8.484603881697694e-07\n",
            "109 8.511132705280033e-07\n",
            "110 8.533752975381503e-07\n",
            "111 8.49044567985402e-07\n",
            "112 8.507452093908796e-07\n",
            "113 8.465494829579256e-07\n",
            "114 8.448091648460831e-07\n",
            "115 8.465792689094087e-07\n",
            "116 8.510483553436643e-07\n",
            "117 8.504117090524232e-07\n",
            "118 8.527641739419778e-07\n",
            "119 8.536862878827378e-07\n",
            "120 8.53150822877069e-07\n",
            "121 8.513137572663254e-07\n",
            "122 8.461339007226343e-07\n",
            "123 8.43582029119716e-07\n",
            "124 8.441361387667712e-07\n",
            "125 8.417167123297986e-07\n",
            "126 8.395992381338147e-07\n",
            "127 8.420835229117074e-07\n",
            "128 8.38353116705548e-07\n",
            "129 8.36893150335527e-07\n",
            "130 8.34592356113717e-07\n",
            "131 8.357903880096273e-07\n",
            "132 8.306296876980923e-07\n",
            "133 8.278433369923732e-07\n",
            "134 8.281119789899094e-07\n",
            "135 8.300481795231462e-07\n",
            "136 8.305731853397447e-07\n",
            "137 8.296271971630631e-07\n",
            "138 8.323743259097682e-07\n",
            "139 8.334523045050446e-07\n",
            "140 8.312723593917326e-07\n",
            "141 8.303833283207496e-07\n",
            "142 8.302503715640341e-07\n",
            "143 8.277970096060017e-07\n",
            "144 8.283294619104709e-07\n",
            "145 8.265965902864991e-07\n",
            "146 8.264250368483772e-07\n",
            "147 8.268195870186901e-07\n",
            "148 8.282128192149685e-07\n",
            "149 8.282643193524564e-07\n",
            "150 8.308095402753679e-07\n",
            "151 8.355222007594421e-07\n",
            "152 8.373438049602555e-07\n",
            "153 8.362294465769082e-07\n",
            "154 8.343963600054849e-07\n",
            "155 8.313382977576111e-07\n",
            "156 8.311768624480464e-07\n",
            "157 8.303126151076867e-07\n",
            "158 8.316355888382532e-07\n",
            "159 8.279934604615846e-07\n",
            "160 8.276179528365901e-07\n",
            "161 8.284921477752505e-07\n",
            "162 8.246397555922158e-07\n",
            "163 8.211101771848917e-07\n",
            "164 8.239431394940766e-07\n",
            "165 8.253551300185791e-07\n",
            "166 8.23597076760052e-07\n",
            "167 8.206340567085135e-07\n",
            "168 8.191869369511551e-07\n",
            "169 8.201238301808189e-07\n",
            "170 8.211317208406399e-07\n",
            "171 8.154399893101072e-07\n",
            "172 8.17792340512824e-07\n",
            "173 8.165037570506684e-07\n",
            "174 8.176242545232526e-07\n",
            "175 8.207507562474348e-07\n",
            "176 8.203668926398677e-07\n",
            "177 8.236735880018387e-07\n",
            "178 8.164097948792914e-07\n",
            "179 8.136723863572115e-07\n",
            "180 8.134133508974628e-07\n",
            "181 8.147485459630843e-07\n",
            "182 8.175460379789001e-07\n",
            "183 8.172684147211839e-07\n",
            "184 8.218485163524747e-07\n",
            "185 8.219626010941283e-07\n",
            "186 8.188429205802095e-07\n",
            "187 8.192335485546209e-07\n",
            "188 8.159330491253058e-07\n",
            "189 8.195091254492581e-07\n",
            "190 8.207771315937862e-07\n",
            "191 8.183430963981664e-07\n",
            "192 8.155965929290687e-07\n",
            "193 8.131172535286169e-07\n",
            "194 8.124432042677654e-07\n",
            "195 8.189127811419894e-07\n",
            "196 8.231469337260933e-07\n",
            "197 8.236807502726151e-07\n",
            "198 8.229725949604472e-07\n",
            "199 8.218186167141539e-07\n",
            "200 8.209459565478028e-07\n",
            "201 8.173928449650703e-07\n",
            "202 8.17020520571532e-07\n",
            "203 8.16946169379662e-07\n",
            "204 8.158017976711562e-07\n",
            "205 8.165559393091826e-07\n",
            "206 8.136148608173244e-07\n",
            "207 8.132548714456789e-07\n",
            "208 8.126324928525719e-07\n",
            "209 8.081852911345777e-07\n",
            "210 8.106982249955763e-07\n",
            "211 8.110364433377981e-07\n",
            "212 8.149707468874112e-07\n",
            "213 8.132380457936961e-07\n",
            "214 8.141000762407202e-07\n",
            "215 8.146738537107012e-07\n",
            "216 8.131401614264178e-07\n",
            "217 8.120465508909547e-07\n",
            "218 8.158838227245724e-07\n",
            "219 8.163631264324067e-07\n",
            "220 8.171570016202168e-07\n",
            "221 8.137056966006639e-07\n",
            "222 8.087923788480111e-07\n",
            "223 8.059447509367601e-07\n",
            "224 8.067885346463299e-07\n",
            "225 8.099532351479866e-07\n",
            "226 8.070660442172084e-07\n",
            "227 8.048789936765388e-07\n",
            "228 8.049302664403513e-07\n",
            "229 8.04049079761171e-07\n",
            "230 7.979839438121417e-07\n",
            "231 7.971340210133349e-07\n",
            "232 7.958627747939317e-07\n",
            "233 7.956806484799017e-07\n",
            "234 7.960720722621772e-07\n",
            "235 7.931672598715522e-07\n",
            "236 7.943719992908882e-07\n",
            "237 7.98335406670958e-07\n",
            "238 7.949091695991228e-07\n",
            "239 7.921660767351568e-07\n",
            "240 7.911077091193874e-07\n",
            "241 7.839836371204001e-07\n",
            "242 7.83769678491808e-07\n",
            "243 7.849246230762219e-07\n",
            "244 7.853846568650624e-07\n",
            "245 7.885689115028072e-07\n",
            "246 7.877578127590823e-07\n",
            "247 7.926753369247308e-07\n",
            "248 7.890548090472294e-07\n",
            "249 7.874601806179271e-07\n",
            "250 7.918114306448842e-07\n",
            "251 7.923733846837422e-07\n",
            "252 7.893063411756884e-07\n",
            "253 7.867209887990612e-07\n",
            "254 7.834432835807092e-07\n",
            "255 7.82045901814854e-07\n",
            "256 7.782696229696739e-07\n",
            "257 7.821809617780673e-07\n",
            "258 7.839780096219329e-07\n",
            "259 7.838615374566871e-07\n",
            "260 7.842044738026743e-07\n",
            "261 7.821041663191863e-07\n",
            "262 7.764264182696934e-07\n",
            "263 7.819097618266824e-07\n",
            "264 7.822911811672384e-07\n",
            "265 7.802729555805854e-07\n",
            "266 7.829563628547476e-07\n",
            "267 7.81046992415213e-07\n",
            "268 7.755588171676209e-07\n",
            "269 7.725315640527697e-07\n",
            "270 7.725311093054188e-07\n",
            "271 7.676339350837225e-07\n",
            "272 7.6914147939533e-07\n",
            "273 7.735271196906979e-07\n",
            "274 7.76110084643733e-07\n",
            "275 7.757264484098414e-07\n",
            "276 7.729531716904603e-07\n",
            "277 7.663942369617871e-07\n",
            "278 7.645384130228194e-07\n",
            "279 7.637476642230467e-07\n",
            "280 7.650902489331202e-07\n",
            "281 7.662651455575542e-07\n",
            "282 7.63055368224741e-07\n",
            "283 7.593466762045864e-07\n",
            "284 7.577074256914784e-07\n",
            "285 7.58773467168794e-07\n",
            "286 7.633021823494346e-07\n",
            "287 7.598539468745003e-07\n",
            "288 7.560478820778371e-07\n",
            "289 7.577496035082731e-07\n",
            "290 7.563830877188593e-07\n",
            "291 7.572200502181659e-07\n",
            "292 7.565968758171948e-07\n",
            "293 7.578277632092068e-07\n",
            "294 7.570887987640162e-07\n",
            "295 7.609296517330222e-07\n",
            "296 7.602602636325173e-07\n",
            "297 7.601414608870982e-07\n",
            "298 7.599206810482428e-07\n",
            "299 7.595366469104192e-07\n",
            "300 7.570735078843427e-07\n",
            "301 7.578029794785834e-07\n",
            "302 7.588939752167789e-07\n",
            "303 7.587296408928523e-07\n",
            "304 7.576441589662863e-07\n",
            "305 7.550367513431411e-07\n",
            "306 7.546225901933212e-07\n",
            "307 7.548903226961556e-07\n",
            "308 7.535766144428635e-07\n",
            "309 7.533070629506256e-07\n",
            "310 7.520295639551478e-07\n",
            "311 7.508267572120531e-07\n",
            "312 7.525467253799434e-07\n",
            "313 7.530539960498572e-07\n",
            "314 7.527199841206311e-07\n",
            "315 7.532788117714517e-07\n",
            "316 7.540671731476323e-07\n",
            "317 7.559060577477794e-07\n",
            "318 7.571510423076688e-07\n",
            "319 7.552310421488073e-07\n",
            "320 7.549842848675326e-07\n",
            "321 7.590256245748606e-07\n",
            "322 7.583016667922493e-07\n",
            "323 7.57871362111473e-07\n",
            "324 7.529340564360609e-07\n",
            "325 7.537972805948812e-07\n",
            "326 7.494455758205731e-07\n",
            "327 7.522162377426866e-07\n",
            "328 7.534689530075411e-07\n",
            "329 7.573096354462905e-07\n",
            "330 7.544750815213774e-07\n",
            "331 7.580352985314676e-07\n",
            "332 7.608108489876031e-07\n",
            "333 7.604108986924984e-07\n",
            "334 7.612769650222617e-07\n",
            "335 7.609845056322229e-07\n",
            "336 7.592364568154153e-07\n",
            "337 7.569841500298935e-07\n",
            "338 7.586880315102462e-07\n",
            "339 7.562416612927336e-07\n",
            "340 7.53771189465624e-07\n",
            "341 7.524735110564507e-07\n",
            "342 7.512317097280174e-07\n",
            "343 7.514545927733707e-07\n",
            "344 7.522895089095982e-07\n",
            "345 7.469419074368489e-07\n",
            "346 7.467488671863975e-07\n",
            "347 7.44046019462985e-07\n",
            "348 7.470565037692722e-07\n",
            "349 7.505178132305446e-07\n",
            "350 7.513063451369817e-07\n",
            "351 7.486829645131365e-07\n",
            "352 7.493040357076097e-07\n",
            "353 7.462277835657005e-07\n",
            "354 7.478292332052661e-07\n",
            "355 7.478400902982685e-07\n",
            "356 7.443591130140703e-07\n",
            "357 7.483559443244303e-07\n",
            "358 7.45865747830976e-07\n",
            "359 7.454332262568641e-07\n",
            "360 7.424596333294176e-07\n",
            "361 7.406404165521963e-07\n",
            "362 7.394519343506545e-07\n",
            "363 7.363595386777888e-07\n",
            "364 7.380168653980945e-07\n",
            "365 7.38249866572005e-07\n",
            "366 7.400407753266336e-07\n",
            "367 7.390538030449534e-07\n",
            "368 7.374141546279134e-07\n",
            "369 7.367785883616307e-07\n",
            "370 7.332036489060556e-07\n",
            "371 7.311692229450273e-07\n",
            "372 7.321292514461675e-07\n",
            "373 7.337052352340834e-07\n",
            "374 7.36893639441405e-07\n",
            "375 7.349062229877745e-07\n",
            "376 7.356001674452273e-07\n",
            "377 7.335935947594407e-07\n",
            "378 7.320170425373362e-07\n",
            "379 7.296559942915337e-07\n",
            "380 7.275011739693582e-07\n",
            "381 7.277624263224425e-07\n",
            "382 7.268474746524589e-07\n",
            "383 7.26793928151892e-07\n",
            "384 7.263015504577197e-07\n",
            "385 7.289966106327483e-07\n",
            "386 7.282686738108168e-07\n",
            "387 7.21355547739222e-07\n",
            "388 7.178841769928113e-07\n",
            "389 7.199201945695677e-07\n",
            "390 7.213706680886389e-07\n",
            "391 7.204831149465463e-07\n",
            "392 7.239362389555026e-07\n",
            "393 7.23420725989854e-07\n",
            "394 7.233131782413693e-07\n",
            "395 7.259724270625156e-07\n",
            "396 7.248956990224542e-07\n",
            "397 7.232611665131117e-07\n",
            "398 7.218193900371261e-07\n",
            "399 7.229758693938493e-07\n",
            "400 7.267299224622548e-07\n",
            "401 7.290103667401127e-07\n",
            "402 7.302134008568828e-07\n",
            "403 7.305087592612836e-07\n",
            "404 7.306304610210645e-07\n",
            "405 7.305284270842094e-07\n",
            "406 7.278570137714269e-07\n",
            "407 7.279865030795918e-07\n",
            "408 7.255218861246249e-07\n",
            "409 7.25123641132086e-07\n",
            "410 7.241919774969574e-07\n",
            "411 7.21145624993369e-07\n",
            "412 7.235464636323741e-07\n",
            "413 7.226453817565925e-07\n",
            "414 7.200437721621711e-07\n",
            "415 7.195305897766957e-07\n",
            "416 7.221862574624538e-07\n",
            "417 7.227105811580259e-07\n",
            "418 7.227830565170734e-07\n",
            "419 7.211055503830721e-07\n",
            "420 7.187295523181092e-07\n",
            "421 7.164956059568794e-07\n",
            "422 7.152810894694994e-07\n",
            "423 7.159278538892977e-07\n",
            "424 7.140923798942822e-07\n",
            "425 7.116066171874991e-07\n",
            "426 7.091409770509927e-07\n",
            "427 7.077421741996659e-07\n",
            "428 7.062421900627669e-07\n",
            "429 7.06511286807654e-07\n",
            "430 7.061611313474714e-07\n",
            "431 7.055624564600294e-07\n",
            "432 7.065812042128528e-07\n",
            "433 7.087350013534888e-07\n",
            "434 7.066024636515067e-07\n",
            "435 7.070634637784678e-07\n",
            "436 7.075501571307541e-07\n",
            "437 7.083259561113664e-07\n",
            "438 7.062051849970885e-07\n",
            "439 7.091829274941119e-07\n",
            "440 7.087019184837118e-07\n",
            "441 7.079497663653456e-07\n",
            "442 7.076553174556466e-07\n",
            "443 7.096480203472311e-07\n",
            "444 7.117138238754706e-07\n",
            "445 7.113030164873635e-07\n",
            "446 7.145995368773583e-07\n",
            "447 7.140814091144421e-07\n",
            "448 7.152447096814285e-07\n",
            "449 7.163552027122932e-07\n",
            "450 7.102469794517674e-07\n",
            "451 7.118719054233225e-07\n",
            "452 7.11698930899729e-07\n",
            "453 7.086892424013058e-07\n",
            "454 7.094325837897486e-07\n",
            "455 7.08611992195074e-07\n",
            "456 7.083679065544857e-07\n",
            "457 7.114159643606399e-07\n",
            "458 7.120471536836703e-07\n",
            "459 7.107884130164166e-07\n",
            "460 7.147220912884222e-07\n",
            "461 7.093167369021103e-07\n",
            "462 7.058584969854564e-07\n",
            "463 7.064130045364436e-07\n",
            "464 7.069467073961277e-07\n",
            "465 7.039676006570517e-07\n",
            "466 7.029278776826686e-07\n",
            "467 7.031026711956656e-07\n",
            "468 7.038978537821095e-07\n",
            "469 7.042295351311623e-07\n",
            "470 7.042062861728482e-07\n",
            "471 7.003263817750849e-07\n",
            "472 7.024264050414786e-07\n",
            "473 7.020244652267138e-07\n",
            "474 7.027315973573423e-07\n",
            "475 7.022318868621369e-07\n",
            "476 7.004965709711541e-07\n",
            "477 7.028626214378164e-07\n",
            "478 7.06032665220846e-07\n",
            "479 7.063472935442405e-07\n",
            "480 7.058086453071155e-07\n",
            "481 7.056669915073144e-07\n",
            "482 7.063533189466398e-07\n",
            "483 7.093264571267355e-07\n",
            "484 7.085505444592854e-07\n",
            "485 7.0887864467295e-07\n",
            "486 7.101644996510004e-07\n",
            "487 7.153852266128524e-07\n",
            "488 7.157628942877636e-07\n",
            "489 7.121216754057969e-07\n",
            "490 7.110440378710337e-07\n",
            "491 7.084839808157994e-07\n",
            "492 7.107348096724309e-07\n",
            "493 7.10541996795655e-07\n",
            "494 7.0929030471234e-07\n",
            "495 7.079685246935696e-07\n",
            "496 7.088407301125699e-07\n",
            "497 7.097431762304041e-07\n",
            "498 7.108357067409088e-07\n",
            "499 7.112523690011585e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycwxAPHZLNST",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Question 12\n",
        "\n",
        "In the code above, why do we have 2 in '2.0*(y_pred - y)`?\n",
        "\n",
        "##Solution:\n",
        "Because the differntiation of last layer \n",
        "\n",
        "d(pow(y_pred-y,2) = 2* y_pred -y\n",
        "\n",
        "## Question 13\n",
        "In the code above, what does `grad_h[h < 0] = 0` signify?\n",
        "##Solution \n",
        "\n",
        "differentiation of relu, for all `h<0` its 0\n",
        "\n",
        "## Question 14\n",
        "In the code above, how many \"epochs\" have we trained the model for? \n",
        "##Solution:\n",
        " 500\n",
        "\n",
        "## Question 15\n",
        "In the code above, if we take the trained model, and run it on fresh  inputs, the trained model will be able to predict fresh output with high accuracy. \n",
        "#Solution : \n",
        "No\n",
        "\n",
        "\n",
        "## Question 16\n",
        "In the code above, if we dont use clone in `grad_h = grad_h_relu.clone()` the model will still train without any issues. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnJ49I07JJI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "No"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN7fWJZDYUi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvJxv5L8YYMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BQn2_c9YifF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzetEOwGYnND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}